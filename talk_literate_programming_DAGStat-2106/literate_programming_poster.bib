@article{Knuth1984,
abstract = {SEMAT (Software Engineering Method and Theory) and its many supporters agree that software engineering is gravely hampered by immature practices. For this reason, SEMAT supports a process to re-found software engineering based on a solid theory, proven principles and best practices that includes a kernel of widelyagreed elements that is extensible for specific uses and addresses both technology and people issues. SEMAT focuses on changing the way people deal with methods and processes, which in turn, will impact our industry, education, research and developer community. In this paper we provide a snapshot of some of SEMATâ€™s results and report on the experiences gained both within the academia and industry.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Knuth, Donald E},
doi = {10.1145/2351676.2351679},
eprint = {arXiv:1011.1669v3},
isbn = {9781450312042},
issn = {00010782},
journal = {The Computer Journal},
keywords = {activity space,alpha,education,kernel,method,practice,software engineering theory,status of semat},
number = {2},
pages = {97--111},
pmid = {20394051},
title = {{Literate Programming}},
url = {http://dl.acm.org/citation.cfm?doid=2351676.2351679},
volume = {27},
year = {1984}
}


@article{Peng2015,
author = {Peng, Roger},
doi = {10.1111/j.1740-9713.2015.00827.x},
issn = {1740-9713},
journal = {Significance},
month = {jun},
number = {3},
pages = {30--32},
title = {{The reproducibility crisis in science: A statistical counterattack}},
url = {http://dx.doi.org/10.1111/j.1740-9713.2015.00827.x},
volume = {12},
year = {2015}
}



@article{OpenScienceCollaboration2015,
abstract = {Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47{\%} of original effect sizes were in the 95{\%} confidence interval of the replication effect size; 39{\%} of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68{\%} with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
annote = {10.1126/science.aac4716},
author = {{Open Science Collaboration}},
doi = {10.1126/science.aac4716},
file = {:Users/sebastiansauer/Documents/Literatur/Methoden{\_}Literatur/Replizierbarkeit{\_}OpenSciene/Repro{\_}psych{\_}Nosek{\_}OSF{\_}Science-2015--.pdf:pdf},
journal = {Science},
month = {aug},
number = {6251},
title = {{Estimating the reproducibility of psychological science}},
url = {http://www.sciencemag.org/content/349/6251/aac4716.abstract},
volume = {349},
year = {2015}
}


@book{xie2013dynamic,
  title={Dynamic Documents with R and knitr},
  author={Xie, Yihui},
  volume={29},
  year={2013},
  publisher={CRC Press}
}